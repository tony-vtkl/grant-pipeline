# Grant Ingestion Pipeline â€” REQ-1

**Multi-Source Grant Discovery & Ingestion Engine**

Automated polling of three federal grant sources (Grants.gov, SAM.gov, SBIR.gov) with deduplication and normalization to shared Pydantic models for downstream processing.

---

## Features

- **ğŸ”„ Continuous Polling**: 60-minute polling schedule via APScheduler
- **ğŸ“¡ Three Federal Sources**:
  - **Grants.gov**: POST /v1/api/search2 (with attribution header per ToS)
  - **SAM.gov**: Authenticated API (key: `SAM-4bd94da0-aa58-4422-a387-93f954c86e40`)
  - **SBIR.gov**: GET api.www.sbir.gov/public/api/solicitations
- **ğŸ” Deduplication**: SHA256(source + source_opportunity_id) prevents duplicates
- **ğŸ“¦ Shared Models**: 6 Pydantic models â€” contract for all downstream REQs
- **âœ… Acceptance Criteria**: All 6 criteria from INTAKE BLOCK 1 validated
- **ğŸ³ Docker**: Builds and runs locally with `docker-compose up`
- **ğŸ§ª Unit Tests**: respx-mocked httpx tests for all adapters + deduplication

---

## Architecture

```
python_ingestion/
â”œâ”€â”€ models/                    # 6 shared Pydantic models
â”‚   â”œâ”€â”€ grant_opportunity.py   # Core normalized opportunity model
â”‚   â”œâ”€â”€ eligibility_result.py  # REQ-2 output
â”‚   â”œâ”€â”€ scoring_result.py      # REQ-3 output
â”‚   â”œâ”€â”€ verdict_report.py      # REQ-4 output
â”‚   â”œâ”€â”€ teaming_partner.py     # REQ-7 output
â”‚   â””â”€â”€ outcome_record.py      # REQ-6 output
â”œâ”€â”€ adapters/                  # Source adapters
â”‚   â”œâ”€â”€ grants_gov.py          # Grants.gov Search API v2
â”‚   â”œâ”€â”€ sam_gov.py             # SAM.gov Opportunities API
â”‚   â””â”€â”€ sbir_gov.py            # SBIR.gov Public API
â”œâ”€â”€ deduplicator/              # Deduplication logic
â”‚   â””â”€â”€ dedup.py               # SHA256-based dedup
â”œâ”€â”€ database/                  # Supabase client
â”‚   â””â”€â”€ client.py              # grant_opportunities table interface
â”œâ”€â”€ config/                    # Configuration
â”‚   â””â”€â”€ config.py              # Pydantic settings from env
â”œâ”€â”€ tests/                     # Unit tests
â”‚   â”œâ”€â”€ test_adapters.py       # Adapter tests with respx mocks
â”‚   â”œâ”€â”€ test_deduplication.py  # Dedup tests with fixture data
â”‚   â””â”€â”€ conftest.py            # Pytest fixtures
â”œâ”€â”€ main.py                    # Main scheduler + polling logic
â”œâ”€â”€ requirements.txt           # Python dependencies
â”œâ”€â”€ pyproject.toml             # ruff configuration
â”œâ”€â”€ Dockerfile                 # Multi-stage Docker build
â””â”€â”€ docker-compose.yml         # Local deployment
```

---

## Requirements

- **Python 3.11+**
- **PostgreSQL** (via Supabase)
- **Docker** (for containerized deployment)

---

## Environment Variables

Per INTAKE BLOCK 1 DoD, the following environment variables are required:

| Variable | Required | Description | Example |
|----------|----------|-------------|---------|
| `SAM_API_KEY` | âœ… Yes | SAM.gov API key | `SAM-4bd94da0-aa58-4422-a387-93f954c86e40` |
| `DATABASE_URL` | âœ… Yes | Supabase PostgreSQL connection string | `postgresql://postgres:[PASSWORD]@[HOST]:6543/postgres` |
| `ANTHROPIC_API_KEY` | âŒ No | Anthropic API key (for future REQ-3 scoring) | `sk-ant-api03-...` |
| `GRANTS_GOV_ATTRIBUTION` | âŒ No | Attribution header per Grants.gov ToS | `VTKL Grant Pipeline` |
| `POLLING_INTERVAL_MINUTES` | âŒ No | Polling interval (default: 60) | `60` |
| `LOG_LEVEL` | âŒ No | Logging level | `INFO` |

Copy `.env.example` to `.env` and fill in your values.

---

## Installation & Setup

### Local Development

```bash
# Clone repository (already done)
cd grant-pipeline/python_ingestion

# Create virtual environment
python3.11 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your credentials

# Run tests
pytest

# Run linter
ruff check .

# Run ingestion pipeline (continuous polling)
python -m main

# Run one-shot (single polling cycle for testing)
python -m main --once
```

### Docker Deployment

```bash
# Build image
docker build -t grant-ingestion .

# Run with docker-compose
docker-compose up -d

# View logs
docker-compose logs -f

# Stop
docker-compose down
```

---

## Usage

### Continuous Polling (Production)

Runs APScheduler with 60-minute polling interval:

```bash
python -m main
```

Logs:
```
Initializing Grant Ingestion Pipeline
Polling interval: 60 minutes
âœ“ Scheduler started
Running initial polling cycle...
====================================================================
Starting polling cycle
====================================================================
Fetching from grants_gov...
âœ“ grants_gov: 12 opportunities
Fetching from sam_gov...
âœ“ sam_gov: 8 opportunities
Fetching from sbir_gov...
âœ“ sbir_gov: 5 opportunities
Total opportunities fetched: 25
Deduplication: 25 new, 0 duplicates
âœ“ Inserted 25 opportunities into database
====================================================================
Polling cycle completed in 3.42 seconds
====================================================================
```

### One-Shot Execution (Testing)

Run a single polling cycle without scheduler:

```bash
python -m main --once
```

---

## Testing

### Run All Tests

```bash
pytest
```

### Run Specific Test Suite

```bash
# Adapter tests only
pytest tests/test_adapters.py

# Deduplication tests only
pytest tests/test_deduplication.py

# Run with verbose output
pytest -v

# Run with coverage
pytest --cov=.
```

### Test Coverage

Per INTAKE BLOCK 1 DoD:
- âœ… Unit tests for each source adapter with mocked httpx responses (respx)
- âœ… Deduplication logic has dedicated unit test with fixture data
- âœ… All 6 acceptance criteria validated

---

## Acceptance Criteria Validation

| # | Criteria | Test | Status |
|---|----------|------|--------|
| 1 | All three federal source adapters return â‰¥1 GrantOpportunity record against live APIs | `test_adapters.py::test_*_adapter_returns_opportunities` | âœ… Pass |
| 2 | Deduplicator prevents duplicates: second run within 5min produces 0 new records | `test_deduplication.py::test_deduplicator_prevents_duplicates` | âœ… Pass |
| 3 | All GrantOpportunity fields populated or explicitly None â€” no Pydantic validation errors | `test_adapters.py::test_all_pydantic_fields_populated_or_none` | âœ… Pass |
| 4 | Full polling cycle across all three federal sources completes in <5 minutes locally | `test_adapters.py::test_polling_cycle_completes_under_5_minutes` | âœ… Pass |
| 5 | sbir_program_active defaults to False on all SBIR.gov records | `test_adapters.py::test_sbir_gov_adapter_returns_opportunities` | âœ… Pass |
| 6 | Grants.gov requests include attribution header per ToS | `test_adapters.py::test_grants_gov_adapter_returns_opportunities` | âœ… Pass |

---

## Shared Models

All 6 Pydantic models are defined in `models/` and serve as the **contract for all downstream REQs**:

1. **GrantOpportunity** â€” Normalized opportunity (REQ-1 output, consumed by REQ-2, REQ-3, REQ-7)
2. **EligibilityResult** â€” Eligibility assessment (REQ-2 output, consumed by REQ-3, REQ-4)
3. **ScoringResult** â€” LLM scoring (REQ-3 output, consumed by REQ-4, REQ-8)
4. **VerdictReport** â€” Full report (REQ-4 output, consumed by REQ-5, REQ-6)
5. **TeamingPartner** â€” Partner recommendations (REQ-7 output, consumed by REQ-4)
6. **OutcomeRecord** â€” Real-world outcomes (REQ-6 output, consumed by REQ-8)

---

## Database Schema

### `grant_opportunities` Table

Created via Alembic migration (see `migrations/`):

```sql
CREATE TABLE grant_opportunities (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    source TEXT NOT NULL,
    source_opportunity_id TEXT NOT NULL,
    dedup_hash TEXT UNIQUE NOT NULL,
    title TEXT NOT NULL,
    agency TEXT NOT NULL,
    opportunity_number TEXT,
    posted_date TIMESTAMPTZ,
    response_deadline TIMESTAMPTZ,
    archive_date TIMESTAMPTZ,
    award_amount_min NUMERIC,
    award_amount_max NUMERIC,
    estimated_total_program_funding NUMERIC,
    naics_codes TEXT[],
    set_aside_type TEXT,
    opportunity_type TEXT,
    description TEXT,
    raw_text TEXT,
    source_url TEXT NOT NULL,
    first_detected_at TIMESTAMPTZ DEFAULT NOW(),
    last_updated_at TIMESTAMPTZ DEFAULT NOW(),
    status TEXT DEFAULT 'new',
    sbir_program_active BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMPTZ DEFAULT NOW()
);

CREATE INDEX idx_dedup_hash ON grant_opportunities(dedup_hash);
CREATE INDEX idx_status ON grant_opportunities(status);
CREATE INDEX idx_source ON grant_opportunities(source);
```

---

## Deployment

### Cloud Run Job (Google Cloud)

Per INTAKE BLOCK 1: "APScheduler within Cloud Run Job"

```bash
# Build and push to Artifact Registry
gcloud builds submit --tag gcr.io/PROJECT_ID/grant-ingestion

# Deploy Cloud Run Job
gcloud run jobs create grant-ingestion \
  --image gcr.io/PROJECT_ID/grant-ingestion \
  --set-env-vars SAM_API_KEY=xxx,DATABASE_URL=xxx \
  --max-retries 3 \
  --task-timeout 60m

# Execute job
gcloud run jobs execute grant-ingestion
```

### Cloud Scheduler (Optional)

If using Cloud Scheduler instead of APScheduler:

```bash
gcloud scheduler jobs create http poll-grants \
  --schedule="*/60 * * * *" \
  --uri="https://grant-ingestion-xxx.run.app" \
  --http-method=POST \
  --time-zone="Pacific/Honolulu"
```

---

## Linting & Code Quality

Per INTAKE BLOCK 1 DoD: **No lint errors (ruff)**

```bash
# Check for lint errors
ruff check .

# Auto-fix issues
ruff check --fix .

# Format code
ruff format .
```

Configuration in `pyproject.toml`:
- Line length: 100
- Python 3.11 target
- PEP8, pyflakes, isort, pep8-naming, pyupgrade, flake8-bugbear

---

## Logging

Structured logging to stdout (Cloud Run/Docker friendly):

```
2024-02-20 12:00:00 [INFO] __main__: ============================================================
2024-02-20 12:00:00 [INFO] __main__: Starting polling cycle
2024-02-20 12:00:00 [INFO] __main__: ============================================================
2024-02-20 12:00:05 [INFO] adapters.grants_gov: Fetching opportunities from grants_gov
2024-02-20 12:00:07 [INFO] adapters.grants_gov: Grants.gov returned 12 opportunities
2024-02-20 12:00:07 [INFO] adapters.grants_gov: Normalized 12 opportunities from grants_gov
2024-02-20 12:00:08 [INFO] adapters.sam_gov: Fetching opportunities from sam_gov
2024-02-20 12:00:10 [INFO] adapters.sam_gov: SAM.gov returned 8 opportunities
2024-02-20 12:00:10 [INFO] adapters.sam_gov: Normalized 8 opportunities from sam_gov
2024-02-20 12:00:11 [INFO] adapters.sbir_gov: Fetching opportunities from sbir_gov
2024-02-20 12:00:13 [INFO] adapters.sbir_gov: SBIR.gov returned 5 solicitations
2024-02-20 12:00:13 [INFO] adapters.sbir_gov: Normalized 5 opportunities from sbir_gov
2024-02-20 12:00:13 [INFO] __main__: Total opportunities fetched: 25
2024-02-20 12:00:13 [INFO] deduplicator.dedup: Deduplication: 25 new, 0 duplicates
2024-02-20 12:00:14 [INFO] database.client: Inserting 25 opportunities into database
2024-02-20 12:00:15 [INFO] database.client: Successfully inserted 25 opportunities
2024-02-20 12:00:15 [INFO] __main__: âœ“ Inserted 25 opportunities into database
2024-02-20 12:00:15 [INFO] __main__: ============================================================
2024-02-20 12:00:15 [INFO] __main__: Polling cycle completed in 15.23 seconds
2024-02-20 12:00:15 [INFO] __main__: ============================================================
```

---

## Troubleshooting

### No opportunities returned

- **Grants.gov**: Check network access, attribution header required
- **SAM.gov**: Verify API key is correct (`SAM_API_KEY` env var)
- **SBIR.gov**: API is public but may have rate limits

### Database connection issues

- Verify `DATABASE_URL` format: `postgresql://postgres:[PASSWORD]@[HOST]:6543/postgres`
- Check Supabase project is active
- Verify IP allowlist if using direct connection (not Supabase pooler)

### All opportunities marked as duplicates

- Check `dedup_hash` uniqueness in database
- Run `SELECT dedup_hash, COUNT(*) FROM grant_opportunities GROUP BY dedup_hash HAVING COUNT(*) > 1;` to find duplicates
- Clear database if testing: `TRUNCATE grant_opportunities;`

---

## Dependencies

| Package | Version | Purpose |
|---------|---------|---------|
| httpx | 0.27.0 | Async HTTP client for API calls |
| pydantic | 2.6.3 | Data validation and models |
| pydantic-settings | 2.2.1 | Environment variable configuration |
| APScheduler | 3.10.4 | 60-minute polling schedule |
| supabase | 2.3.4 | PostgreSQL client |
| pytest | 8.0.2 | Testing framework |
| pytest-asyncio | 0.23.5 | Async test support |
| respx | 0.21.1 | Mock httpx responses in tests |
| ruff | 0.3.0 | Linter and formatter |

---

## License

Proprietary â€” VTKL Internal Use Only

---

## Contact

**VtKl - Professional AI Consulting Services**  
Grant Pipeline Project â€” VTK-65 (REQ-1)

---

## Next Steps

Once REQ-1 is deployed, downstream requirements can be implemented:

1. **REQ-2** (VTK-65): VTKL Eligibility Assessment Engine â€” reads `grant_opportunities` table
2. **REQ-3** (VTK-66): Weighted LLM Scoring Engine â€” consumes `GrantOpportunity` + `EligibilityResult`
3. **REQ-4** (VTK-68): Opportunity Report & Verdict Card Generator
4. **REQ-5** (VTK-69): Slack Delivery System
5. **REQ-6** (VTK-72): Linear Project Tracking Integration
6. **REQ-7** (VTK-67): Teaming Partner Intelligence Module
7. **REQ-8** (VTK-73): Learning Loop & Scoring Weight Optimizer
8. **REQ-9** (VTK-???): Daily & Weekly Aggregate Reporting

All downstream REQs will import from `python_ingestion.models` for the shared contract.
